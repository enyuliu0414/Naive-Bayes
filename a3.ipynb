{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tagname>\n",
    "    <H1>Assignment 3</H1>\n",
    "    <p>For my standard Naive Bayes algorithm, I identify 1000 frequently occurring words and generate a word dictionary which contain word name and ID number (from 1 to 1000). Then create a word metrics which have 1000 columns and each column count the number of words occurs in the corresponding word dictionary.  In order to avoid the zero-value problem when calculating the proportion of a certain word in a certain category. I added Laplace smoothing. The numerator will plus the coefficient 1 and the denominator plus the size of the whole dictionary which is 1000.</p>\n",
    "    <p>I created four lists to store the number of unique word occours in Class A, B, E and V respectively. According to the assumption of Naive Bayes that each attribute is independent of each other, the text of abstracts can be viewed as a sequence composed of many words, and the words in the abstract can be used to build a polynomial model. Finally, the multiplication of the probability of each word is rewritten into the mode of log (probability) summation to optimize our Naive Bayes algorithm.Then return the highest-ranking class through sorting. This is the process how I create the trainning model. In my first version I did not include pre-processing of the texts. After apply a 10 folds cross-validation results in a score of 90.5%. </p>\n",
    "    <p>“Stop words” in the text such as' am','is','are','was','were','she','he'. We need to delete those words that are not helpful for the further analysis of the article in the pre-processing stage. Moreover, the repeated occurrence of stop words is highly likely to cause model misclassification. After deleting the “stop words”, the accuracy was slightly improved, reaching 90.7%.</p>\n",
    "    <p>Different attribute-value affect the accuracy a lot. My next extension is to obtain an attribute-value representation that identifies 12000 unique words in the training set. Adjust the number of selected most frequency occurring words through the results of 10 folds cross-validation. I found that the accuracy increased with the increase of the number of unique words. When about 12,000 most frequently occurring words were selected as unique words vector, the cross-validation result seems to be the maximum. After this threshold is exceeded, the accuracy begins to decrease. Compared with only choose 1000 most frequently occurring words, the accuracy of this predicting model raised significantly which score is 95.525%</p>\n",
    "    <p>In my final extension, I used a method called TF-IDF to improve the accuracy of the model. This method optimizes the algorithm by giving weights to words. If a word appears in many paragraphs which IDF value should be lower. On the other hand, if a word appears relatively few occurrences, its IDF value should be high. The formula is log (Y / W + 1) where Y is the total number of documents in the whole CSV file, and W is the number of documents containing the term. In the end, the prediction model obtained by multiplying TF and IDF which makes the model more accurate. The accuracy of the final extended version reaches 96.225%</p>\n",
    "</tagname>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'trg.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "train_filename = \"trg\"\n",
    "test_filename = \"tst\"\n",
    "output_filename = \"eliu092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(test_filename, output_filename, trained_model):\n",
    "    test_data = np.genfromtxt(f'{test_filename}.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "    output = open(f'{output_filename}.csv', encoding='utf8', mode='w')\n",
    "    output.write('id,class\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        abstract = test_data[i][1].replace('\"', ' ').split()\n",
    "        prediction = predict(abstract, trained_model)\n",
    "        output.write(f'{i + 1},{prediction}\\n')\n",
    "    output.close()\n",
    "\n",
    "def predict(abstract,trained_model):\n",
    "        top1000_dictionary_id, label_prob, conditional_probabilities_A,conditional_probabilities_B,conditional_probabilities_E,conditional_probabilities_V = trained_model\n",
    "        word_vector = np.zeros(1000, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "                \n",
    "        \n",
    "        p_label_A = np.log(label_prob['A'])\n",
    "        p_A = sum(word_vector * np.log(conditional_probabilities_A)) + p_label_A\n",
    "    \n",
    "        p_label_B = np.log(label_prob['B'])\n",
    "        p_B = sum(word_vector * np.log(conditional_probabilities_B)) + p_label_B\n",
    "    \n",
    "        p_label_E = np.log(label_prob['E'])\n",
    "        p_E = sum(word_vector * np.log(conditional_probabilities_E)) + p_label_E\n",
    "    \n",
    "        p_label_V = np.log(label_prob['V'])\n",
    "        p_V = sum(word_vector * np.log(conditional_probabilities_V)) + p_label_V\n",
    "        prediction = {\"A\": p_A, \"B\": p_B, \"E\": p_E, \"V\": p_V}\n",
    "        prediction_array = prediction.items()\n",
    "        p = sorted(prediction_array, key=lambda key_value: key_value[1], reverse=True)\n",
    "        p = p[0]\n",
    "        return p[0]\n",
    "\n",
    "def Convert(tup, dictionary):\n",
    "    i = 0\n",
    "    for a, b in tup:\n",
    "        dictionary.setdefault(a,i)\n",
    "        i += 1\n",
    "    return dictionary\n",
    "\n",
    "def NaiveBayes(data):\n",
    "    count_apperance = {}\n",
    "    top1000 = []\n",
    "    top1000_dictionary = {}\n",
    "    for i in range(len(data)):\n",
    "        abstract =  data[i][2]\n",
    "        abstract = abstract.replace('\"', ' ').split()\n",
    "        for word in abstract:\n",
    "            if word in count_apperance:\n",
    "                count_apperance[word] += 1\n",
    "            else:\n",
    "                count_apperance[word] = 1\n",
    "    count_apperance_array = count_apperance.items()    \n",
    "    descending_sorting = sorted(count_apperance_array, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(0,1000):\n",
    "        top1000.append(descending_sorting[i])\n",
    "    top1000_dictionary_id =  Convert(top1000, top1000_dictionary)\n",
    "    new_data = np.zeros((len(data), 1000))\n",
    "    for i in range(len(data)):\n",
    "        abstract = data[i][2].split()\n",
    "        word_vector = np.zeros(1000, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "        new_data[i] = word_vector\n",
    "# class probability calculation\n",
    "    label_count = {}\n",
    "    for i in range(len(data)):\n",
    "        label = data[i][1]\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "    label_prob = {}\n",
    "    for i in label_count.keys():\n",
    "        label_prob[i] = label_count[i] / len(data)\n",
    "        \n",
    "    A = np.zeros(1000, dtype=int)\n",
    "    B = np.zeros(1000, dtype=int)\n",
    "    E = np.zeros(1000, dtype=int)\n",
    "    V = np.zeros(1000, dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == 'B':\n",
    "            B = np.sum([B,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'A':\n",
    "            A = np.sum([A,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'E':\n",
    "            E = np.sum([E,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'V':\n",
    "            V = np.sum([V,new_data[i]],axis=0)\n",
    "    A_count = sum(A)\n",
    "    B_count = sum(B)\n",
    "    E_count = sum(E)\n",
    "    V_count = sum(V)\n",
    "# calculate conditional probabilities\n",
    "    conditional_probability_A = []\n",
    "    conditional_probability_B = []\n",
    "    conditional_probability_E = []\n",
    "    conditional_probability_V = []\n",
    "    for a in range(len(A)):\n",
    "        probability_A = (A[a]+1)/(A_count+1000)\n",
    "        conditional_probability_A.append(probability_A)\n",
    "    for b in range(len(B)):\n",
    "        probability_B = (B[b]+1)/(B_count+1000)\n",
    "        conditional_probability_B.append(probability_B)\n",
    "    for e in range(len(E)):\n",
    "        probability_E = (E[e]+1)/(E_count+1000)\n",
    "        conditional_probability_E.append(probability_E)\n",
    "    for v in range(len(V)):\n",
    "        probability_V = (V[v]+1)/(V_count+1000)\n",
    "        conditional_probability_V.append(probability_V)\n",
    "    return top1000_dictionary_id,label_prob, conditional_probability_A, conditional_probability_B, conditional_probability_E, conditional_probability_V\n",
    "\n",
    "\n",
    "\n",
    "trained_model = NaiveBayes(data)\n",
    "write_output(test_filename, output_filename, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = 10\n",
    "def test(abstract, trained_model):\n",
    "    correct_predict = 0\n",
    "    for i in range(len(abstract)):\n",
    "        text = abstract[i][2].replace('\"', ' ').split()\n",
    "        prediction = predict(text, trained_model)\n",
    "        if prediction == abstract[i][1]:\n",
    "            correct_predict += 1\n",
    "    acc =  correct_predict / len(abstract)\n",
    "    return acc\n",
    "        \n",
    "\n",
    "def cross_validation(data,cv):\n",
    "    acc_list = []\n",
    "    each_fold_data_size = len(data) // cv\n",
    "    start_index = list(range(0, len(data), each_fold_data_size))\n",
    "    for i in range(cv):\n",
    "        start = start_index[i]\n",
    "        test_data= data[start: start + each_fold_data_size]\n",
    "        train_data = np.append(data[:start], data[start + each_fold_data_size:], axis=0)\n",
    "        model = NaiveBayes(train_data)\n",
    "        acc = test(test_data, model)\n",
    "        acc_list.append(acc)\n",
    "    mean_score = sum(acc_list) / len(acc_list)\n",
    "                       \n",
    "                       \n",
    "    return mean_score\n",
    "\n",
    "acc_temp = cross_validation(data, cv)\n",
    "acc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'trg.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "\n",
    "train_filename = \"trg\"\n",
    "test_filename = \"tst\"\n",
    "output_filename = \"eliu092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(test_filename, output_filename, trained_model):\n",
    "    test_data = np.genfromtxt(f'{test_filename}.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "    output = open(f'{output_filename}.csv', encoding='utf8', mode='w')\n",
    "    output.write('id,class\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        abstract = test_data[i][1].replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        prediction = predict(abstract, trained_model)\n",
    "        output.write(f'{i + 1},{prediction}\\n')\n",
    "    output.close()\n",
    "\n",
    "def predict(abstract,trained_model):\n",
    "        top1000_dictionary_id, label_prob, conditional_probabilities_A,conditional_probabilities_B,conditional_probabilities_E,conditional_probabilities_V = trained_model\n",
    "        word_vector = np.zeros(1000, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "                \n",
    "        \n",
    "        p_label_A = np.log(label_prob['A'])\n",
    "        p_A = sum(word_vector * np.log(conditional_probabilities_A)) + p_label_A\n",
    "    \n",
    "        p_label_B = np.log(label_prob['B'])\n",
    "        p_B = sum(word_vector * np.log(conditional_probabilities_B)) + p_label_B\n",
    "    \n",
    "        p_label_E = np.log(label_prob['E'])\n",
    "        p_E = sum(word_vector * np.log(conditional_probabilities_E)) + p_label_E\n",
    "    \n",
    "        p_label_V = np.log(label_prob['V'])\n",
    "        p_V = sum(word_vector * np.log(conditional_probabilities_V)) + p_label_V\n",
    "        prediction = {\"A\": p_A, \"B\": p_B, \"E\": p_E, \"V\": p_V}\n",
    "        prediction_array = prediction.items()\n",
    "        p = sorted(prediction_array, key=lambda key_value: key_value[1], reverse=True)\n",
    "        p = p[0]\n",
    "        return p[0]\n",
    "\n",
    "def preprocessing(abstract):       \n",
    "    stop_words = ['am','is','are','was','were','she','he','they','these','his','their','those','there','her','them','it','its','have','has',\n",
    "                  'at','in','on','by','for','to','the','a','an','such','as','be','but','however','that','come','go','from','this','we','most',\n",
    "                  'since','of','next','then','and','or','may','might','when','alse','some','with','which','who','whose','between','been','do','did','done'\n",
    "                 ]\n",
    "    abstract = [word for word in abstract if word not in stop_words]\n",
    "    return abstract  \n",
    "def Convert(tup, dictionary):\n",
    "    i = 0\n",
    "    for a, b in tup:\n",
    "        dictionary.setdefault(a,i)\n",
    "        i += 1\n",
    "    return dictionary\n",
    "\n",
    "def NaiveBayes(data):\n",
    "    count_apperance = {}\n",
    "    top1000 = []\n",
    "    top1000_dictionary = {}\n",
    "    for i in range(len(data)):\n",
    "        abstract =  data[i][2]\n",
    "        abstract = abstract.replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        for word in abstract:\n",
    "            if word in count_apperance:\n",
    "                count_apperance[word] += 1\n",
    "            else:\n",
    "                count_apperance[word] = 1\n",
    "    count_apperance_array = count_apperance.items()    \n",
    "    descending_sorting = sorted(count_apperance_array, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(0,1000):\n",
    "        top1000.append(descending_sorting[i])\n",
    "    top1000_dictionary_id =  Convert(top1000, top1000_dictionary)\n",
    "    new_data = np.zeros((len(data), 1000))\n",
    "    for i in range(len(data)):\n",
    "        abstract = data[i][2].split()\n",
    "        word_vector = np.zeros(1000, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "        new_data[i] = word_vector\n",
    "# class probability calculation\n",
    "    label_count = {}\n",
    "    for i in range(len(data)):\n",
    "        label = data[i][1]\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "    label_prob = {}\n",
    "    for i in label_count.keys():\n",
    "        label_prob[i] = label_count[i] / len(data)\n",
    "        \n",
    "    A = np.zeros(1000, dtype=int)\n",
    "    B = np.zeros(1000, dtype=int)\n",
    "    E = np.zeros(1000, dtype=int)\n",
    "    V = np.zeros(1000, dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == 'B':\n",
    "            B = np.sum([B,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'A':\n",
    "            A = np.sum([A,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'E':\n",
    "            E = np.sum([E,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'V':\n",
    "            V = np.sum([V,new_data[i]],axis=0)\n",
    "    A_count = sum(A)\n",
    "    B_count = sum(B)\n",
    "    E_count = sum(E)\n",
    "    V_count = sum(V)\n",
    "# calculate conditional probabilities\n",
    "    conditional_probability_A = []\n",
    "    conditional_probability_B = []\n",
    "    conditional_probability_E = []\n",
    "    conditional_probability_V = []\n",
    "    for a in range(len(A)):\n",
    "        probability_A = (A[a]+1)/(A_count+1000)\n",
    "        conditional_probability_A.append(probability_A)\n",
    "    for b in range(len(B)):\n",
    "        probability_B = (B[b]+1)/(B_count+1000)\n",
    "        conditional_probability_B.append(probability_B)\n",
    "    for e in range(len(E)):\n",
    "        probability_E = (E[e]+1)/(E_count+1000)\n",
    "        conditional_probability_E.append(probability_E)\n",
    "    for v in range(len(V)):\n",
    "        probability_V = (V[v]+1)/(V_count+1000)\n",
    "        conditional_probability_V.append(probability_V)\n",
    "    return top1000_dictionary_id,label_prob, conditional_probability_A, conditional_probability_B, conditional_probability_E, conditional_probability_V\n",
    "\n",
    "\n",
    "\n",
    "trained_model = NaiveBayes(data)\n",
    "write_output(test_filename, output_filename, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = 10\n",
    "def test(abstract, trained_model):\n",
    "    correct_predict = 0\n",
    "    for i in range(len(abstract)):\n",
    "        text = abstract[i][2].replace('\"', ' ').split()\n",
    "        word = preprocessing(text)\n",
    "        prediction = predict(word, trained_model)\n",
    "        if prediction == abstract[i][1]:\n",
    "            correct_predict += 1\n",
    "    acc =  correct_predict / len(abstract)\n",
    "    return acc\n",
    "        \n",
    "\n",
    "def cross_validation(data,cv):\n",
    "    acc_list = []\n",
    "    each_fold_data_size = len(data) // cv\n",
    "    start_index = list(range(0, len(data), each_fold_data_size))\n",
    "    for i in range(cv):\n",
    "        start = start_index[i]\n",
    "        test_data= data[start: start + each_fold_data_size]\n",
    "        train_data = np.append(data[:start], data[start + each_fold_data_size:], axis=0)\n",
    "        model = NaiveBayes(train_data)\n",
    "        acc = test(test_data, model)\n",
    "        acc_list.append(acc)\n",
    "    mean_score = sum(acc_list) / len(acc_list)\n",
    "                       \n",
    "                       \n",
    "    return mean_score\n",
    "\n",
    "acc_temp = cross_validation(data, cv)\n",
    "acc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'trg.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "\n",
    "train_filename = \"trg\"\n",
    "test_filename = \"tst\"\n",
    "output_filename = \"eliu092_update\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(test_filename, output_filename, trained_model):\n",
    "    test_data = np.genfromtxt(f'{test_filename}.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "    output = open(f'{output_filename}.csv', encoding='utf8', mode='w')\n",
    "    output.write('id,class\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        abstract = test_data[i][1].replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        prediction = predict(abstract, trained_model)\n",
    "        output.write(f'{i + 1},{prediction}\\n')\n",
    "    output.close()\n",
    "\n",
    "def predict(abstract,trained_model):\n",
    "        top1000_dictionary_id, label_prob, conditional_probabilities_A,conditional_probabilities_B,conditional_probabilities_E,conditional_probabilities_V = trained_model\n",
    "        global count\n",
    "        word_vector = np.zeros(count, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "                \n",
    "        \n",
    "        p_label_A = np.log(label_prob['A'])\n",
    "        p_A = sum(word_vector * np.log(conditional_probabilities_A)) + p_label_A\n",
    "    \n",
    "        p_label_B = np.log(label_prob['B'])\n",
    "        p_B = sum(word_vector * np.log(conditional_probabilities_B)) + p_label_B\n",
    "    \n",
    "        p_label_E = np.log(label_prob['E'])\n",
    "        p_E = sum(word_vector * np.log(conditional_probabilities_E)) + p_label_E\n",
    "    \n",
    "        p_label_V = np.log(label_prob['V'])\n",
    "        p_V = sum(word_vector * np.log(conditional_probabilities_V)) + p_label_V\n",
    "        prediction = {\"A\": p_A, \"B\": p_B, \"E\": p_E, \"V\": p_V}\n",
    "        prediction_array = prediction.items()\n",
    "        p = sorted(prediction_array, key=lambda key_value: key_value[1], reverse=True)\n",
    "        p = p[0]\n",
    "        return p[0]\n",
    "\n",
    "def preprocessing(abstract):       \n",
    "    stop_words = ['am','is','are','was','were','she','he','they','these','his','their','those','there','her','them','it','its','have','has',\n",
    "                  'at','in','on','by','for','to','the','a','an','such','as','be','but','however','that','come','go','from','this','we','most',\n",
    "                  'since','of','next','then','and','or','may','might','when','alse','some','with','which','who','whose','1','2','3','also',\n",
    "                  '4','5','6','7','8','9','0','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','between','been','do','did','done',\n",
    "                  'p','q','r','s','t','u','v','w','x','y','z'\n",
    "                 ]\n",
    "    abstract = [word for word in abstract if word not in stop_words]\n",
    "    return abstract  \n",
    "def Convert(tup, dictionary):\n",
    "    i = 0\n",
    "    for a, b in tup:\n",
    "        dictionary.setdefault(a,i)\n",
    "        i += 1\n",
    "    return dictionary\n",
    "\n",
    "def NaiveBayes(data):\n",
    "    count_apperance = {}\n",
    "    top1000 = []\n",
    "    top1000_dictionary = {}\n",
    "    for i in range(len(data)):\n",
    "        abstract =  data[i][2]\n",
    "        abstract = abstract.replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        for word in abstract:\n",
    "            if word in count_apperance:\n",
    "                count_apperance[word] += 1\n",
    "            else:\n",
    "                count_apperance[word] = 1\n",
    "    count_apperance_array = count_apperance.items()    \n",
    "    descending_sorting = sorted(count_apperance_array, key = lambda x:x[1], reverse = True)\n",
    "    global count\n",
    "#     count = len(descending_sorting)\n",
    "    count = 12000\n",
    "    for i in range(0,count):\n",
    "        top1000.append(descending_sorting[i])\n",
    "    top1000_dictionary_id =  Convert(top1000, top1000_dictionary)\n",
    "    new_data = np.zeros((len(data), count))\n",
    "    for i in range(len(data)):\n",
    "        abstract = data[i][2].split()\n",
    "        word_vector = np.zeros(count, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "        new_data[i] = word_vector\n",
    "# class probability calculation\n",
    "    label_count = {}\n",
    "    for i in range(len(data)):\n",
    "        label = data[i][1]\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "    label_prob = {}\n",
    "    for i in label_count.keys():\n",
    "        label_prob[i] = label_count[i] / len(data)\n",
    "        \n",
    "    A = np.zeros(count, dtype=int)\n",
    "    B = np.zeros(count, dtype=int)\n",
    "    E = np.zeros(count, dtype=int)\n",
    "    V = np.zeros(count, dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == 'B':\n",
    "            B = np.sum([B,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'A':\n",
    "            A = np.sum([A,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'E':\n",
    "            E = np.sum([E,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'V':\n",
    "            V = np.sum([V,new_data[i]],axis=0)\n",
    "    A_count = sum(A)\n",
    "    B_count = sum(B)\n",
    "    E_count = sum(E)\n",
    "    V_count = sum(V)\n",
    "# calculate conditional probabilities\n",
    "    conditional_probability_A = []\n",
    "    conditional_probability_B = []\n",
    "    conditional_probability_E = []\n",
    "    conditional_probability_V = []\n",
    "    for a in range(len(A)):\n",
    "        probability_A = (A[a]+1)/(A_count+count)\n",
    "        conditional_probability_A.append(probability_A)\n",
    "    for b in range(len(B)):\n",
    "        probability_B = (B[b]+1)/(B_count+count)\n",
    "        conditional_probability_B.append(probability_B)\n",
    "    for e in range(len(E)):\n",
    "        probability_E = (E[e]+1)/(E_count+count)\n",
    "        conditional_probability_E.append(probability_E)\n",
    "    for v in range(len(V)):\n",
    "        probability_V = (V[v]+1)/(V_count+count)\n",
    "        conditional_probability_V.append(probability_V)\n",
    "    return top1000_dictionary_id,label_prob, conditional_probability_A, conditional_probability_B, conditional_probability_E, conditional_probability_V\n",
    "\n",
    "\n",
    "\n",
    "trained_model = NaiveBayes(data)\n",
    "write_output(test_filename, output_filename, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95525"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = 10\n",
    "def test(abstract, trained_model):\n",
    "    correct_predict = 0\n",
    "    for i in range(len(abstract)):\n",
    "        text = abstract[i][2].replace('\"', ' ').split()\n",
    "        word = preprocessing(text)\n",
    "        prediction = predict(word, trained_model)\n",
    "        if prediction == abstract[i][1]:\n",
    "            correct_predict += 1\n",
    "    acc =  correct_predict / len(abstract)\n",
    "    return acc\n",
    "        \n",
    "\n",
    "def cross_validation(data,cv):\n",
    "    acc_list = []\n",
    "    each_fold_data_size = len(data) // cv\n",
    "    start_index = list(range(0, len(data), each_fold_data_size))\n",
    "    for i in range(cv):\n",
    "        start = start_index[i]\n",
    "        test_data= data[start: start + each_fold_data_size]\n",
    "        train_data = np.append(data[:start], data[start + each_fold_data_size:], axis=0)\n",
    "        model = NaiveBayes(train_data)\n",
    "        acc = test(test_data, model)\n",
    "        acc_list.append(acc)\n",
    "    mean_score = sum(acc_list) / len(acc_list)\n",
    "                       \n",
    "                       \n",
    "    return mean_score\n",
    "\n",
    "acc_temp = cross_validation(data, cv)\n",
    "acc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'trg.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "train_filename = \"trg\"\n",
    "test_filename = \"tst\"\n",
    "output_filename = \"eliu092_final_ff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(test_filename, output_filename, trained_model):\n",
    "    test_data = np.genfromtxt(f'{test_filename}.csv', delimiter=',', encoding='utf8', dtype=None)[1:]\n",
    "    output = open(f'{output_filename}.csv', encoding='utf8', mode='w')\n",
    "    output.write('id,class\\n')\n",
    "    for i in range(len(test_data)):\n",
    "        abstract = test_data[i][1].replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        prediction = predict(abstract, trained_model)\n",
    "        output.write(f'{i + 1},{prediction}\\n')\n",
    "    output.close()\n",
    "\n",
    "def predict(abstract,trained_model):\n",
    "        top1000_dictionary_id, label_prob, conditional_probabilities_A,conditional_probabilities_B,conditional_probabilities_E,conditional_probabilities_V = trained_model\n",
    "        global count\n",
    "        word_vector = np.zeros(count, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "                \n",
    "        \n",
    "        p_label_A = np.log(label_prob['A'])\n",
    "        p_A = sum(word_vector * np.log(conditional_probabilities_A)) + p_label_A\n",
    "    \n",
    "        p_label_B = np.log(label_prob['B'])\n",
    "        p_B = sum(word_vector * np.log(conditional_probabilities_B)) + p_label_B\n",
    "    \n",
    "        p_label_E = np.log(label_prob['E'])\n",
    "        p_E = sum(word_vector * np.log(conditional_probabilities_E)) + p_label_E\n",
    "    \n",
    "        p_label_V = np.log(label_prob['V'])\n",
    "        p_V = sum(word_vector * np.log(conditional_probabilities_V)) + p_label_V\n",
    "        prediction = {\"A\": p_A, \"B\": p_B, \"E\": p_E, \"V\": p_V}\n",
    "        prediction_array = prediction.items()\n",
    "        p = sorted(prediction_array, key=lambda key_value: key_value[1], reverse=True)\n",
    "        p = p[0]\n",
    "        return p[0]\n",
    "\n",
    "def preprocessing(abstract):       \n",
    "    stop_words = ['am','is','are','was','were','she','he','they','these','his','their','those','there','her','them','it','its','have','has',\n",
    "                  'at','in','on','by','for','to','the','a','an','such','as','be','but','however','that','come','go','from','this','we','most',\n",
    "                  'since','of','next','then','and','or','may','might','when','alse','some','with','which','who','whose','also','between','been','do','did','done'\n",
    "                 ]\n",
    "    abstract = [word for word in abstract if word not in stop_words]\n",
    "    return abstract  \n",
    "def Convert(tup, dictionary):\n",
    "    i = 0\n",
    "    for a, b in tup:\n",
    "        dictionary.setdefault(a,i)\n",
    "        i += 1\n",
    "    return dictionary\n",
    "\n",
    "def NaiveBayes(data):\n",
    "    count_apperance = {}\n",
    "    top1000 = []\n",
    "    top1000_dictionary = {}\n",
    "    for i in range(len(data)):\n",
    "        abstract =  data[i][2]\n",
    "        abstract = abstract.replace('\"', ' ').split()\n",
    "        abstract = preprocessing(abstract)\n",
    "        for word in abstract:\n",
    "            if word in count_apperance:\n",
    "                count_apperance[word] += 1\n",
    "            else:\n",
    "                count_apperance[word] = 1\n",
    "    count_apperance_array = count_apperance.items()    \n",
    "    descending_sorting = sorted(count_apperance_array, key = lambda x:x[1], reverse = True)\n",
    "    global count\n",
    "#     count = len(descending_sorting)\n",
    "    count = 12000\n",
    "    for i in range(0,count):\n",
    "        top1000.append(descending_sorting[i])\n",
    "    top1000_dictionary_id =  Convert(top1000, top1000_dictionary)\n",
    "    new_data = np.zeros((len(data), count))\n",
    "    for i in range(len(data)):\n",
    "        abstract = data[i][2].split()\n",
    "        word_vector = np.zeros(count, dtype=int)\n",
    "        for word in abstract:\n",
    "            if word in top1000_dictionary_id:\n",
    "                word_vector[top1000_dictionary_id[word]] += 1\n",
    "        new_data[i] = word_vector\n",
    "#TF IDF\n",
    "    len_data_instance = len(new_data)\n",
    "    new_data = np.transpose(new_data)\n",
    "    for i in range(count):\n",
    "        count_frequency = len([1 for j in new_data[i] if j != 0])\n",
    "        new_data[i] = new_data[i] * np.log(len_data_instance / (count_frequency + 1) )\n",
    "    new_data = np.transpose(new_data)\n",
    "    \n",
    "    \n",
    "# class probability calculation\n",
    "    label_count = {}\n",
    "    for i in range(len(data)):\n",
    "        label = data[i][1]\n",
    "        if label in label_count:\n",
    "            label_count[label] += 1\n",
    "        else:\n",
    "            label_count[label] = 1\n",
    "    label_prob = {}\n",
    "    for i in label_count.keys():\n",
    "        label_prob[i] = label_count[i] / len(data)\n",
    "        \n",
    "    A = np.zeros(count, dtype=int)\n",
    "    B = np.zeros(count, dtype=int)\n",
    "    E = np.zeros(count, dtype=int)\n",
    "    V = np.zeros(count, dtype=int)\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == 'B':\n",
    "            B = np.sum([B,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'A':\n",
    "            A = np.sum([A,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'E':\n",
    "            E = np.sum([E,new_data[i]],axis=0)\n",
    "        if data[i][1] == 'V':\n",
    "            V = np.sum([V,new_data[i]],axis=0)\n",
    "    A_count = sum(A)\n",
    "    B_count = sum(B)\n",
    "    E_count = sum(E)\n",
    "    V_count = sum(V)\n",
    "# calculate conditional probabilities\n",
    "    conditional_probability_A = []\n",
    "    conditional_probability_B = []\n",
    "    conditional_probability_E = []\n",
    "    conditional_probability_V = []\n",
    "    for a in range(len(A)):\n",
    "        probability_A = (A[a]+1)/(A_count+count)\n",
    "        conditional_probability_A.append(probability_A)\n",
    "    for b in range(len(B)):\n",
    "        probability_B = (B[b]+1)/(B_count+count)\n",
    "        conditional_probability_B.append(probability_B)\n",
    "    for e in range(len(E)):\n",
    "        probability_E = (E[e]+1)/(E_count+count)\n",
    "        conditional_probability_E.append(probability_E)\n",
    "    for v in range(len(V)):\n",
    "        probability_V = (V[v]+1)/(V_count+count)\n",
    "        conditional_probability_V.append(probability_V)\n",
    "    return top1000_dictionary_id,label_prob, conditional_probability_A, conditional_probability_B, conditional_probability_E, conditional_probability_V\n",
    "\n",
    "\n",
    "\n",
    "trained_model = NaiveBayes(data)\n",
    "write_output(test_filename, output_filename, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96225"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = 10\n",
    "def test(abstract, trained_model):\n",
    "    correct_predict = 0\n",
    "    for i in range(len(abstract)):\n",
    "        text = abstract[i][2].replace('\"', ' ').split()\n",
    "        word = preprocessing(text)\n",
    "        prediction = predict(word, trained_model)\n",
    "        if prediction == abstract[i][1]:\n",
    "            correct_predict += 1\n",
    "    acc =  correct_predict / len(abstract)\n",
    "    return acc\n",
    "        \n",
    "\n",
    "def cross_validation(data,cv):\n",
    "    acc_list = []\n",
    "    each_fold_data_size = len(data) // cv\n",
    "    start_index = list(range(0, len(data), each_fold_data_size))\n",
    "    for i in range(cv):\n",
    "        start = start_index[i]\n",
    "        test_data= data[start: start + each_fold_data_size]\n",
    "        train_data = np.append(data[:start], data[start + each_fold_data_size:], axis=0)\n",
    "        model = NaiveBayes(train_data)\n",
    "        acc = test(test_data, model)\n",
    "        acc_list.append(acc)\n",
    "    mean_score = sum(acc_list) / len(acc_list)\n",
    "                       \n",
    "                       \n",
    "    return mean_score\n",
    "\n",
    "acc_temp = cross_validation(data, cv)\n",
    "acc_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
